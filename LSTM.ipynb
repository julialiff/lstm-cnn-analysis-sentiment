{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global Imports\n",
    "Keras - deep learning library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import csv\n",
    "import datetime\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, LSTM\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing dataset\n",
    "Imports a certain number of lines from the dataset and adds the sentiment column, according to the number of stars given in the review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_id</th>\n",
       "      <th>stars</th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>vkVSCC7xljjrAI4UGfnKEQ</td>\n",
       "      <td>5</td>\n",
       "      <td>Super simple place but amazing nonetheless. It...</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>n6QzIUObkYshz4dz2QRJTw</td>\n",
       "      <td>5</td>\n",
       "      <td>Small unassuming place that changes their menu...</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>MV3CcKScW05u5LVfF6ok0g</td>\n",
       "      <td>5</td>\n",
       "      <td>Lester's is located in a beautiful neighborhoo...</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>IXvOzsEMYtiJI0CARmj77Q</td>\n",
       "      <td>4</td>\n",
       "      <td>Love coming here. Yes the place always needs t...</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>L_9BTb55X0GDtThi6GlZ6w</td>\n",
       "      <td>4</td>\n",
       "      <td>Had their chocolate almond croissant and it wa...</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                review_id  stars  \\\n",
       "0  vkVSCC7xljjrAI4UGfnKEQ      5   \n",
       "1  n6QzIUObkYshz4dz2QRJTw      5   \n",
       "2  MV3CcKScW05u5LVfF6ok0g      5   \n",
       "3  IXvOzsEMYtiJI0CARmj77Q      4   \n",
       "4  L_9BTb55X0GDtThi6GlZ6w      4   \n",
       "\n",
       "                                                text sentiment  \n",
       "0  Super simple place but amazing nonetheless. It...       pos  \n",
       "1  Small unassuming place that changes their menu...       pos  \n",
       "2  Lester's is located in a beautiful neighborhoo...       pos  \n",
       "3  Love coming here. Yes the place always needs t...       pos  \n",
       "4  Had their chocolate almond croissant and it wa...       pos  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filename = '/home/ec2-user/SageMaker/y2.csv'\n",
    "# Importing 30000 lines from the CSV\n",
    "data = pd.read_csv(filename, error_bad_lines=False)[:30000]\n",
    "# Removing unnecessary columns\n",
    "data = data.drop(['user_id','business_id','date','funny','cool','useful'],axis=1)\n",
    "# Adding the sentiment column\n",
    "data['sentiment'] = ['pos' if (x > 3) else 'neutral' if (x == 3) else 'neg' for x in data['stars']]\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing\n",
    "Remove special characters and tokenize the text, to break the sentences into tokens, without pontuaction or spacing.\n",
    "The tokenizer is a class from Keras that prepares text for deep learning.\n",
    "\n",
    "**Arguments:**\n",
    "- **num_words:** the maximum number of words to keep, based on word frequency. Only the most common num_words words will be kept.\n",
    "- **lower:** boolean. Whether to convert the texts to lowercase.\n",
    "- **split:** str. Separator for word splitting.\n",
    "\n",
    "**Outputs:**\n",
    "- **word_counts:** A dictionary of words and their counts.\n",
    "- **word_docs:** A dictionary of words and how many documents each appeared in.\n",
    "- **word_index:** A dictionary of words and their uniquely assigned integers.\n",
    "- **document_count:** An integer count of the total number of documents that were used to fit the Tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove special characters\n",
    "data['text'] = data['text'].apply((lambda x: re.sub('[^a-zA-z0-9\\s]','',x)))\n",
    "\n",
    "# Tokenizes the text (2500 words, turn lowercase and split on spacing)\n",
    "tokenizer = Tokenizer(num_words=2500, lower=True,split=' ')\n",
    "\n",
    "tokenizer.fit_on_texts(data['text'].values)\n",
    "X = tokenizer.texts_to_sequences(data['text'].values)\n",
    "X = pad_sequences(X) # Ensures that all sequences have the same length\n",
    "\n",
    "# Summarize what was learned\n",
    "print(tokenizer.word_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM Model\n",
    "Model, with:\n",
    "- 1 Embedding Layer\n",
    "- 1 LSTM Layer\n",
    "- 1 Dense Layer with softmax activation (normalizes the vector into a probability distribution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/keras/backend/mxnet_backend.py:89: UserWarning: MXNet Backend: `unroll=False` is not supported yet in RNN. Since the input_shape is known, setting `unroll=True` and continuing the execution.More Details - https://github.com/awslabs/keras-apache-mxnet/tree/master/docs/mxnet_backend/using_rnn_with_mxnet_backend.md\n",
      "  train_symbol = func(*args, **kwargs)\n",
      "/home/ec2-user/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/keras/backend/mxnet_backend.py:92: UserWarning: MXNet Backend: `unroll=False` is not supported yet in RNN. Since the input_shape is known, setting `unroll=True` and continuing the execution.More Details - https://github.com/awslabs/keras-apache-mxnet/tree/master/docs/mxnet_backend/using_rnn_with_mxnet_backend.md\n",
      "  test_symbol = func(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_4 (Embedding)      (None, 911, 128)          320000    \n",
      "_________________________________________________________________\n",
      "lstm_4 (LSTM)                (None, 300)               514800    \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 3)                 903       \n",
      "=================================================================\n",
      "Total params: 835,703\n",
      "Trainable params: 835,703\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "embed_dim = 128 # Size of the vocabulary\n",
    "lstm_out = 300 # size of the LSTM layer output\n",
    "batch_size= 32 # Iterate the training data in batches of size 32\n",
    "\n",
    "## Building the LSTM network\n",
    "\n",
    "model = Sequential() # To create sequential models\n",
    "model.add(Embedding(2500, embed_dim,input_length = X.shape[1])) #transform each word in an integer\n",
    "model.add(LSTM(lstm_out)) # LSTM layer\n",
    "model.add(Dropout(0.6)) # Applies Dropout to the input (randomly setting a fraction rate of input units to 0 at each update during training time, which helps prevent overfitting)\n",
    "model.add(Dense(3, activation='softmax')) # Dense Layer\n",
    "model.compile(optimizer='adam', # required argument\n",
    "              loss = 'categorical_crossentropy', # required argument\n",
    "              metrics = ['accuracy']) # judge the performance of the model\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "80% of the samples used for training and 20% used for testing.\n",
    "Running with only 1 epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 21600 samples, validate on 2400 samples\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/mxnet/module/bucketing_module.py:408: UserWarning: Optimizer created manually outside Module but rescale_grad is not normalized to 1.0/batch_size/num_workers (1.0 vs. 0.03125). Is this intended?\n",
      "  force_init=force_init)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15872/21600 [=====================>........] - ETA: 9:41 - loss: 0.7042 - acc: 0.7284"
     ]
    }
   ],
   "source": [
    "Y = pd.get_dummies(data['sentiment']).values\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size = 0.20)\n",
    "\n",
    "# Trains the model for a given number of epochs (iterations on a dataset).\n",
    "model.fit(X_train, Y_train, batch_size = batch_size, epochs = 1, verbose = 1, validation_split=0.1)\n",
    "\n",
    "# Measuring score and accuracy on validation set\n",
    "score,acc = model.evaluate(X_test, Y_test, verbose = 0, batch_size = batch_size)\n",
    "print(\"Loss score: %.2f\" % (score))\n",
    "print(\"Test Accuracy: %.2f\" % (acc*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction\n",
    "Predicting all the training data, that is 20% of the input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:00:47.635268\n",
      "[[ 0.33294457  0.33262652  0.33442888]\n",
      " [ 0.33391529  0.33356822  0.33251652]\n",
      " [ 0.33155784  0.33269012  0.33575207]\n",
      " ..., \n",
      " [ 0.33614403  0.33158755  0.33226845]\n",
      " [ 0.33493704  0.33306998  0.33199292]\n",
      " [ 0.33371884  0.33333763  0.33294347]]\n"
     ]
    }
   ],
   "source": [
    "start = datetime.datetime.now()\n",
    "classes = model.predict(X_test, batch_size=32)\n",
    "end = datetime.datetime.now()\n",
    "print(end - start)\n",
    "print(classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_mxnet_p36",
   "language": "python",
   "name": "conda_mxnet_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
